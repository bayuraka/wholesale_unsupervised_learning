---
title: "Wholesale Unsupervised Learning"
author: "Bayu Raka Janasri"
date: "6/21/2021"
output:
  html_document:
    theme: flatly
    higlight: zenburn
    toc: true
    toc_float:
      collapsed: true
    df_print: paged
    number_sections : True
---
![](wholesale.jpeg)

Source Dataset :https://www.kaggle.com/binovi/wholesale-customers-data-set

# Import Library
```{r, message=FALSE}
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(FactoMineR)
library(factoextra)
library(scales)
library(GGally)
```

# Read Data
```{r}
wholesale <- read.csv("wholesale.csv")
glimpse(wholesale)
```

Description : 

1) `FRESH`: annual spending (m.u.) on fresh products (Continuous).

2) `MILK`: annual spending (m.u.) on milk products (Continuous).

3) `GROCERY`: annual spending (m.u.)on grocery products (Continuous).

4) `FROZEN`: annual spending (m.u.)on frozen products (Continuous).

5) `DETERGENTS_PAPER`: annual spending (m.u.) on detergents and paper products (Continuous).

6) `DELICATESSEN`: annual spending (m.u.)on and delicatessen products (Continuous).

7) `CHANNEL`: Channel - 1 = Horeca (Hotel/Restaurant/Cafe) or 2 = Retail channel (Nominal).

8) `REGION` : 1 =  Lisbon / 2 = Porto / 3 =  other region.

**Check first 5 rows**
```{r}
head(wholesale)
```
**Check Missing Value**
```{r}
colSums(is.na(wholesale))
```
There is no missing value at our dataset.

# Data Preprocessing

**Change data type**

```{r}
wholesale <- wholesale %>% 
          mutate(Channel = as.factor(Channel),
                 Region = as.factor(Region))
```

**Check Summary**
```{r}
summary(wholesale)
```
**Data Visualization**
```{r}
ggplot(gather(wholesale %>% select_if(is.numeric)), aes(value)) + 
    geom_histogram(bins = 10,fill="firebrick") + 
    facet_wrap(~key, scales = 'free_x',nrow=3) +
  theme_bw()
```

We can see based on summary and data visualization. We found that our max value for each column is higher from the mean and median value. We can conclude that the data have outlier. We will check later using PCA to found the outliers.

# Modelling

## PCA

Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.

```{r}
quanti <- wholesale %>% 
  select_if(is.numeric) %>% 
  colnames()

quantivar <- which(colnames(wholesale) %in% quanti)

quali <- wholesale %>% 
  select_if(is.factor) %>% 
  colnames()

qualivar <- which(colnames(wholesale) %in% quali)
```


**FactoMineR package for function PCA()**

```{r}
wholesale_pca <- PCA(wholesale, 
                scale.unit = T, #scaling
                quali.sup = qualivar, #data categorical
                graph = F, # not showing graph
                ncp = 5)

wholesale_pca$eig
```

Let’s visualize the percentage of variances captured by each dimensions.

```{r}
fviz_eig(wholesale_pca, ncp = 6, 
         addlabels = T, main = "Variance explained by each dimensions")
```
**Check Summary**

```{r}
summary(wholesale_pca)
```
Through the PCA, I can retain some informative PC (high in cumulative variance) to perform dimension reduction. By doing this, I can reduce the dimension of the variables while also retaining as much information as possible.

Based on Plot and Summary above, I would like to retain at least 80% of the data. Therefore I am going to choose Dim 1-3


## Visualization PCA

### Individual Factor Map

We can see outliers by using this plot.

```{r}
plot.PCA(wholesale_pca, 
         choix = "ind", 
         invisible = "quali", 
         select = "contrib 5", 
         habillage = 1)
```

Through the individual plot of PCA, dim 1 could cover **44.08%** variance of data.

We also found the 5 outlier to be (depends on the menu Channel):

2 from Hotel/Restaurant/Cafe : 184, 326.

3 from Retail : 48, 62, 86.

### Variables Factor Map

To represent more than two components tha variables will be positioned inside the circle of correlation. If the variable is closer to the circle (outside), that means the variable can reconstruct it better from the first two components. If the variable is closed to the center of the plot (inside), that means the variable is less important for the two components.

```{r}
plot.PCA(wholesale_pca,
         choix = "var")
```
Insight:

- PC1 mostly sum of two variables: Grocery, Detergents_Paper.

- PC2 mostly sum of two variables : Frozen, Fresh.


```{r}
fviz_contrib(X = wholesale_pca,
             choice = "var",
             axes = 1)
```

Grocery, Detergents_Paper, Milk are most items contributed to dimention 1.

```{r}
fviz_contrib(X = wholesale_pca,
             choice = "var",
             axes = 2)
```

Frozen, Fresh, Delicassen are most items contributed to dimention 2.

## Clustering

**Choose numeric column**
```{r}
wholesale_num <- wholesale %>% select_if(is.numeric)
```

**Scale dataset**
```{r}
wholesale_num_scale <- scale(wholesale_num)
```

**Finding K-Optimum using Elbow Method**

Choosing the number of clusters using elbow method is arbitrary. The rule of thumb is we choose the number of cluster in the area of “bend of an elbow”, where the graph is total within sum of squares start to stagnate with the increase of the number of clusters.
```{r}
fviz_nbclust(wholesale_num_scale, kmeans, method = "wss", k.max = 30)+ labs(subtitle = "Elbow method") + theme_bw()
```
Using the elbow method, we know that 5 cluster is good enough since there is no significant decline in total within-cluster sum of squares on higher number of clusters. This method may be not enough since the optimal number of clusters is vague.

**K-Means Clustering**

```{r}
RNGkind(sample.kind = "Rounding")
set.seed(100)

# k-means clustering
wholesale_kmeans <- kmeans(wholesale_num_scale, centers = 5)
```


## Visualization Clustering

```{r}
fviz_cluster(wholesale_kmeans, data=wholesale_num_scale)
```
**Cluster Profilling**

```{r}
#put label cluster into dataset
wholesale$cluster <- wholesale_kmeans$cluster

wholesale1 <- wholesale[,3:9]

# make profilling with summarise data
wholesale1 %>% 
  group_by(cluster) %>% 
  summarise_all(mean)
```
Based on result above, below are profiles on each clusters:

- Cluster 1 spending highest at `Fresh` and lowest at `Delicassen`.
- Cluster 2 spending highest at `Fresh` and Frozen but lowest at `Detergents_Paper`.
- Cluster 3 spending highest at `Fresh` and lowest at `Detergents_Paper`.
- Cluster 4 spending highest at `Grocery` and lowest at `Frozen`.
- Cluster 5 spending highest at `Grocery` and lowest at `Delicassen`.

# Conclusion

After exploring our dataset by using `PCA` and `K-Means` for clustering, we are able to conclude that:

 - Wholesale item can be separated into 5 clusters.
 - We can get available stock at our supplier by looking based on regional or their channel.
 